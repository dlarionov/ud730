{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 2\n",
    "------------\n",
    "\n",
    "Previously in `1_notmnist.ipynb`, we created a pickle with formatted datasets for training, development and testing on the [notMNIST dataset](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html).\n",
    "\n",
    "The goal of this assignment is to progressively train deeper and more accurate models using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19456,
     "status": "ok",
     "timestamp": 1449847956073,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "0ddb1607-1fc4-4ddb-de28-6c7ab7fb0c33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19723,
     "status": "ok",
     "timestamp": 1449847956364,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "2ba0fc75-1487-4ace-a562-cf81cae82793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCLVqyQ5vPPH"
   },
   "source": [
    "We're first going to train a multinomial logistic regression using simple gradient descent.\n",
    "\n",
    "TensorFlow works like this:\n",
    "* First you describe the computation that you want to see performed: what the inputs, the variables, and the operations look like. These get created as nodes over a computation graph. This description is all contained within the block below:\n",
    "\n",
    "      with graph.as_default():\n",
    "          ...\n",
    "\n",
    "* Then you can run the operations on this graph as many times as you want by calling `session.run()`, providing it outputs to fetch from the graph that get returned. This runtime operation is all contained in the block below:\n",
    "\n",
    "      with tf.Session(graph=graph) as session:\n",
    "          ...\n",
    "\n",
    "Let's load all the data into TensorFlow and build the computation graph corresponding to our training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Nfv39qvtvOl_"
   },
   "outputs": [],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "train_subset = 10000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    # Load the training, validation and test data into constants that are\n",
    "    # attached to the graph.\n",
    "    tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "    tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    # These are the parameters that we are going to be training. The weight\n",
    "    # matrix will be initialized using random values following a (truncated)\n",
    "    # normal distribution. The biases get initialized to zero.\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "    # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "    # it's very common, and it can be optimized). We take the average of this\n",
    "    # cross-entropy across all training examples: that's our loss.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    # We are going to find the minimum of this loss using gradient descent.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    # These are not part of training, but merely here so that we can report\n",
    "    # accuracy figures as we train.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KQcL4uqISHjP"
   },
   "source": [
    "Let's run this computation and iterate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 9
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 57454,
     "status": "ok",
     "timestamp": 1449847994134,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "z2cjdenH869W",
    "outputId": "4c037ba1-b526-4d8e-e632-91e2a0333267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 13.725228\n",
      "Training accuracy: 12.8%\n",
      "Validation accuracy: 17.5%\n",
      "Loss at step 100: 2.397572\n",
      "Training accuracy: 71.5%\n",
      "Validation accuracy: 70.2%\n",
      "Loss at step 200: 1.910007\n",
      "Training accuracy: 74.6%\n",
      "Validation accuracy: 72.9%\n",
      "Loss at step 300: 1.646322\n",
      "Training accuracy: 76.0%\n",
      "Validation accuracy: 73.5%\n",
      "Loss at step 400: 1.470644\n",
      "Training accuracy: 76.9%\n",
      "Validation accuracy: 73.9%\n",
      "Loss at step 500: 1.341581\n",
      "Training accuracy: 77.7%\n",
      "Validation accuracy: 74.2%\n",
      "Loss at step 600: 1.240844\n",
      "Training accuracy: 78.1%\n",
      "Validation accuracy: 74.6%\n",
      "Loss at step 700: 1.159202\n",
      "Training accuracy: 78.7%\n",
      "Validation accuracy: 74.8%\n",
      "Loss at step 800: 1.091320\n",
      "Training accuracy: 79.0%\n",
      "Validation accuracy: 75.1%\n",
      "Test accuracy: 82.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 801\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # This is a one-time operation which ensures the parameters get initialized as\n",
    "    # we described in the graph: random weights for the matrix, zeros for the\n",
    "    # biases. \n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "        # and get the loss value and the training predictions returned as numpy\n",
    "        # arrays.\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "        if (step % 100 == 0):\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('Training accuracy: %.1f%%' % accuracy(predictions, train_labels[:train_subset, :]))\n",
    "            # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "            # just to get that one numpy array. Note that it recomputes all its graph\n",
    "            # dependencies.\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x68f-hxRGm3H"
   },
   "source": [
    "Let's now switch to stochastic gradient descent training instead, which is much faster.\n",
    "\n",
    "The graph will be similar, except that instead of holding all the training data into a constant node, we create a `Placeholder` node which will be fed actual data at every call of `session.run()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "qhPMzWYRGrzM"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XmVZESmtG4JH"
   },
   "source": [
    "Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 6
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 66292,
     "status": "ok",
     "timestamp": 1449848003013,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "FoF91pknG_YW",
    "outputId": "d255c80e-954d-4183-ca1c-c7333ce91d0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 23.339506\n",
      "Minibatch accuracy: 1.6%\n",
      "Validation accuracy: 7.8%\n",
      "Test accuracy: 7.2%\n",
      "Minibatch loss at step 500: 1.609578\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 75.5%\n",
      "Test accuracy: 82.6%\n",
      "Minibatch loss at step 1000: 1.061576\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 76.6%\n",
      "Test accuracy: 84.1%\n",
      "Minibatch loss at step 1500: 0.875493\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 76.9%\n",
      "Test accuracy: 84.2%\n",
      "Minibatch loss at step 2000: 0.743584\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 77.3%\n",
      "Test accuracy: 84.9%\n",
      "Minibatch loss at step 2500: 1.184945\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 77.0%\n",
      "Test accuracy: 84.9%\n",
      "Minibatch loss at step 3000: 1.196900\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 77.7%\n",
      "Test accuracy: 85.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {\n",
    "            tf_train_dataset : batch_data, \n",
    "            tf_train_labels : batch_labels\n",
    "        }\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7omWxtvLLxik"
   },
   "source": [
    "---\n",
    "Problem\n",
    "-------\n",
    "\n",
    "Turn the logistic regression example with SGD into a 1-hidden layer neural network with rectified linear units [nn.relu()](https://www.tensorflow.org/versions/r0.7/api_docs/python/nn.html#relu) and 1024 hidden nodes. This model should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 784)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def initialize_parameters():\n",
    "    parameters = {}\n",
    "    layers_dims = [ image_size**2, 1024, num_labels ]\n",
    "    \n",
    "    for l in range(1, len(layers_dims)):\n",
    "        parameters['W' + str(l)] = tf.Variable(tf.truncated_normal([layers_dims[l-1], layers_dims[l]]))\n",
    "        parameters['b' + str(l)] = tf.Variable(tf.zeros([layers_dims[l]]))\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    A = X\n",
    "    L = len(parameters) // 2 + 1 # number of layers in the neural network\n",
    "    \n",
    "    # linear[1] -> relu -> liner[2] -> ... -> relu -> linear[L-1]\n",
    "    for l in range(1, L):\n",
    "        W = parameters['W' + str(l)]\n",
    "        b = parameters['b' + str(l)]\n",
    "        Z = tf.add(tf.matmul(A, W), b)\n",
    "        if (l < L - 1):\n",
    "            A = tf.nn.relu(Z)\n",
    "               \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "learning_rate= 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size**2))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset) \n",
    "    \n",
    "    parameters = initialize_parameters()\n",
    "    \n",
    "    logits = forward_propagation(tf_train_dataset, parameters)\n",
    "        \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "      \n",
    "    train_prediction = tf.nn.softmax(forward_propagation(tf_train_dataset, parameters))\n",
    "    valid_prediction = tf.nn.softmax(forward_propagation(tf_valid_dataset, parameters))\n",
    "    test_prediction = tf.nn.softmax(forward_propagation(tf_test_dataset, parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 278.537018\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 24.7%\n",
      "Test accuracy: 27.3%\n",
      "Minibatch loss at step 500: 25.782450\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.2%\n",
      "Test accuracy: 86.3%\n",
      "Minibatch loss at step 1000: 4.762920\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.1%\n",
      "Test accuracy: 88.1%\n",
      "Minibatch loss at step 1500: 11.619221\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 80.2%\n",
      "Test accuracy: 87.1%\n",
      "Minibatch loss at step 2000: 1.936922\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 80.1%\n",
      "Test accuracy: 87.9%\n",
      "Minibatch loss at step 2500: 5.839372\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 81.5%\n",
      "Test accuracy: 88.7%\n",
      "Minibatch loss at step 3000: 4.861326\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.2%\n",
      "Test accuracy: 88.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "costs = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {\n",
    "            tf_train_dataset : batch_data, \n",
    "            tf_train_labels : batch_labels\n",
    "        }\n",
    "        _, step_cost, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, step_cost))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "        if (step % 5 == 0):\n",
    "            costs.append(step_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16a9c438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(np.squeeze(costs))\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per tens)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "2_fullyconnected.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
