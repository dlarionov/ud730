{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 2\n",
    "------------\n",
    "\n",
    "Previously in `1_notmnist.ipynb`, we created a pickle with formatted datasets for training, development and testing on the [notMNIST dataset](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html).\n",
    "\n",
    "The goal of this assignment is to progressively train deeper and more accurate models using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19456,
     "status": "ok",
     "timestamp": 1449847956073,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "0ddb1607-1fc4-4ddb-de28-6c7ab7fb0c33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19723,
     "status": "ok",
     "timestamp": 1449847956364,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "2ba0fc75-1487-4ace-a562-cf81cae82793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCLVqyQ5vPPH"
   },
   "source": [
    "We're first going to train a multinomial logistic regression using simple gradient descent.\n",
    "\n",
    "TensorFlow works like this:\n",
    "* First you describe the computation that you want to see performed: what the inputs, the variables, and the operations look like. These get created as nodes over a computation graph. This description is all contained within the block below:\n",
    "\n",
    "      with graph.as_default():\n",
    "          ...\n",
    "\n",
    "* Then you can run the operations on this graph as many times as you want by calling `session.run()`, providing it outputs to fetch from the graph that get returned. This runtime operation is all contained in the block below:\n",
    "\n",
    "      with tf.Session(graph=graph) as session:\n",
    "          ...\n",
    "\n",
    "Let's load all the data into TensorFlow and build the computation graph corresponding to our training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Nfv39qvtvOl_"
   },
   "outputs": [],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "train_subset = 10000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "  tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KQcL4uqISHjP"
   },
   "source": [
    "Let's run this computation and iterate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 9
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 57454,
     "status": "ok",
     "timestamp": 1449847994134,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "z2cjdenH869W",
    "outputId": "4c037ba1-b526-4d8e-e632-91e2a0333267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 18.408834\n",
      "Training accuracy: 8.6%\n",
      "Validation accuracy: 11.1%\n",
      "Loss at step 100: 2.384980\n",
      "Training accuracy: 72.0%\n",
      "Validation accuracy: 70.4%\n",
      "Loss at step 200: 1.891818\n",
      "Training accuracy: 75.0%\n",
      "Validation accuracy: 72.7%\n",
      "Loss at step 300: 1.626513\n",
      "Training accuracy: 76.1%\n",
      "Validation accuracy: 73.5%\n",
      "Loss at step 400: 1.451177\n",
      "Training accuracy: 77.1%\n",
      "Validation accuracy: 74.0%\n",
      "Loss at step 500: 1.324303\n",
      "Training accuracy: 77.8%\n",
      "Validation accuracy: 74.4%\n",
      "Loss at step 600: 1.225874\n",
      "Training accuracy: 78.3%\n",
      "Validation accuracy: 74.7%\n",
      "Loss at step 700: 1.146118\n",
      "Training accuracy: 78.9%\n",
      "Validation accuracy: 74.8%\n",
      "Loss at step 800: 1.079804\n",
      "Training accuracy: 79.5%\n",
      "Validation accuracy: 74.8%\n",
      "Test accuracy: 82.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 801\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # This is a one-time operation which ensures the parameters get initialized as\n",
    "  # we described in the graph: random weights for the matrix, zeros for the\n",
    "  # biases. \n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the loss value and the training predictions returned as numpy\n",
    "    # arrays.\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    if (step % 100 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Training accuracy: %.1f%%' % accuracy(predictions, train_labels[:train_subset, :]))\n",
    "      # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "      # just to get that one numpy array. Note that it recomputes all its graph\n",
    "      # dependencies.\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x68f-hxRGm3H"
   },
   "source": [
    "Let's now switch to stochastic gradient descent training instead, which is much faster.\n",
    "\n",
    "The graph will be similar, except that instead of holding all the training data into a constant node, we create a `Placeholder` node which will be fed actual data at every call of `session.run()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "qhPMzWYRGrzM"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XmVZESmtG4JH"
   },
   "source": [
    "Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 6
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 66292,
     "status": "ok",
     "timestamp": 1449848003013,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "FoF91pknG_YW",
    "outputId": "d255c80e-954d-4183-ca1c-c7333ce91d0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 17.718943\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 15.1%\n",
      "Minibatch loss at step 500: 1.778924\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 1000: 1.167906\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 1500: 0.795983\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 76.9%\n",
      "Minibatch loss at step 2000: 0.984890\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 2500: 1.230332\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 75.7%\n",
      "Minibatch loss at step 3000: 1.393707\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 77.7%\n",
      "Test accuracy: 85.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7omWxtvLLxik"
   },
   "source": [
    "---\n",
    "Problem\n",
    "-------\n",
    "\n",
    "Turn the logistic regression example with SGD into a 1-hidden layer neural network with rectified linear units [nn.relu()](https://www.tensorflow.org/versions/r0.7/api_docs/python/nn.html#relu) and 1024 hidden nodes. This model should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 784)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameteres(n_x, n_h, n_y):\n",
    "    \n",
    "    W1 = tf.Variable(tf.truncated_normal([n_x, n_h]))\n",
    "    b1 = tf.Variable(tf.zeros([n_h]))\n",
    "    W2 = tf.Variable(tf.truncated_normal([n_h, n_y]))\n",
    "    b2 = tf.Variable(tf.zeros([n_y]))\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    Z1 = tf.add(tf.matmul(X, W1), b1)\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(A1, W2), b2)\n",
    "        \n",
    "    return Z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "batch_size = 128\n",
    "learning_rate= 0.05\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size**2))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset) \n",
    "    \n",
    "    parameters = initialize_parameteres(n_x=image_size**2, n_h=1000, n_y=num_labels)\n",
    "    \n",
    "    logits = forward_propagation(tf_train_dataset, parameters)\n",
    "        \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    # optimizer = tf.train.AdamOptimizer(0.005).minimize(loss)\n",
    "  \n",
    "    train_prediction = tf.nn.softmax(forward_propagation(tf_train_dataset, parameters))\n",
    "    valid_prediction = tf.nn.softmax(forward_propagation(tf_valid_dataset, parameters))\n",
    "    test_prediction = tf.nn.softmax(forward_propagation(tf_test_dataset, parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 392.555054\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 19.0%\n",
      "Test accuracy: 21.6%\n",
      "Minibatch loss at step 1000: 15.437103\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 80.0%\n",
      "Test accuracy: 87.4%\n",
      "Minibatch loss at step 2000: 4.487266\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 82.2%\n",
      "Test accuracy: 89.0%\n",
      "Minibatch loss at step 3000: 10.539413\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 82.6%\n",
      "Test accuracy: 89.6%\n",
      "Minibatch loss at step 4000: 8.125216\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 83.5%\n",
      "Test accuracy: 90.3%\n",
      "Minibatch loss at step 5000: 4.222659\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 83.7%\n",
      "Test accuracy: 90.4%\n",
      "Minibatch loss at step 6000: 4.191556\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 83.7%\n",
      "Test accuracy: 90.2%\n",
      "Minibatch loss at step 7000: 2.431044\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 84.3%\n",
      "Test accuracy: 91.1%\n",
      "Minibatch loss at step 8000: 2.621127\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 84.3%\n",
      "Test accuracy: 91.2%\n",
      "Minibatch loss at step 9000: 4.627687\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 83.3%\n",
      "Test accuracy: 90.0%\n",
      "Minibatch loss at step 10000: 2.999788\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 84.5%\n",
      "Test accuracy: 91.2%\n",
      "Training time: 84.02880620956421\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_steps = 10001\n",
    "start = time.time()\n",
    "costs = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {\n",
    "            tf_train_dataset : batch_data, \n",
    "            tf_train_labels : batch_labels\n",
    "        }\n",
    "        _, step_cost, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 1000 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, step_cost))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "        if (step % 5 == 0):\n",
    "            costs.append(step_cost)\n",
    "print(\"Training time: {}\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhJQgREQkof8HeTUUlm1rs40AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1082d1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(np.squeeze(costs))\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per tens)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "2_fullyconnected.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
