{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 2\n",
    "------------\n",
    "\n",
    "Previously in `1_notmnist.ipynb`, we created a pickle with formatted datasets for training, development and testing on the [notMNIST dataset](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html).\n",
    "\n",
    "The goal of this assignment is to progressively train deeper and more accurate models using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19456,
     "status": "ok",
     "timestamp": 1449847956073,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "0ddb1607-1fc4-4ddb-de28-6c7ab7fb0c33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19723,
     "status": "ok",
     "timestamp": 1449847956364,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "2ba0fc75-1487-4ace-a562-cf81cae82793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCLVqyQ5vPPH"
   },
   "source": [
    "We're first going to train a multinomial logistic regression using simple gradient descent.\n",
    "\n",
    "TensorFlow works like this:\n",
    "* First you describe the computation that you want to see performed: what the inputs, the variables, and the operations look like. These get created as nodes over a computation graph. This description is all contained within the block below:\n",
    "\n",
    "      with graph.as_default():\n",
    "          ...\n",
    "\n",
    "* Then you can run the operations on this graph as many times as you want by calling `session.run()`, providing it outputs to fetch from the graph that get returned. This runtime operation is all contained in the block below:\n",
    "\n",
    "      with tf.Session(graph=graph) as session:\n",
    "          ...\n",
    "\n",
    "Let's load all the data into TensorFlow and build the computation graph corresponding to our training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Nfv39qvtvOl_"
   },
   "outputs": [],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "train_subset = 10000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    # Load the training, validation and test data into constants that are\n",
    "    # attached to the graph.\n",
    "    tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "    tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    # These are the parameters that we are going to be training. The weight\n",
    "    # matrix will be initialized using random values following a (truncated)\n",
    "    # normal distribution. The biases get initialized to zero.\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "    # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "    # it's very common, and it can be optimized). We take the average of this\n",
    "    # cross-entropy across all training examples: that's our loss.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    # We are going to find the minimum of this loss using gradient descent.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    # These are not part of training, but merely here so that we can report\n",
    "    # accuracy figures as we train.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KQcL4uqISHjP"
   },
   "source": [
    "Let's run this computation and iterate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 9
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 57454,
     "status": "ok",
     "timestamp": 1449847994134,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "z2cjdenH869W",
    "outputId": "4c037ba1-b526-4d8e-e632-91e2a0333267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 18.901485\n",
      "Training accuracy: 11.8%\n",
      "Validation accuracy: 13.7%\n",
      "Loss at step 100: 2.364669\n",
      "Training accuracy: 70.8%\n",
      "Validation accuracy: 70.2%\n",
      "Loss at step 200: 1.871592\n",
      "Training accuracy: 74.0%\n",
      "Validation accuracy: 73.0%\n",
      "Loss at step 300: 1.607424\n",
      "Training accuracy: 75.3%\n",
      "Validation accuracy: 74.0%\n",
      "Loss at step 400: 1.432018\n",
      "Training accuracy: 76.3%\n",
      "Validation accuracy: 74.6%\n",
      "Loss at step 500: 1.305428\n",
      "Training accuracy: 77.0%\n",
      "Validation accuracy: 74.9%\n",
      "Loss at step 600: 1.209350\n",
      "Training accuracy: 77.7%\n",
      "Validation accuracy: 75.1%\n",
      "Loss at step 700: 1.133087\n",
      "Training accuracy: 78.1%\n",
      "Validation accuracy: 75.2%\n",
      "Loss at step 800: 1.070359\n",
      "Training accuracy: 78.5%\n",
      "Validation accuracy: 75.3%\n",
      "Test accuracy: 83.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 801\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # This is a one-time operation which ensures the parameters get initialized as\n",
    "    # we described in the graph: random weights for the matrix, zeros for the\n",
    "    # biases. \n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "        # and get the loss value and the training predictions returned as numpy\n",
    "        # arrays.\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "        if (step % 100 == 0):\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('Training accuracy: %.1f%%' % accuracy(predictions, train_labels[:train_subset, :]))\n",
    "            # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "            # just to get that one numpy array. Note that it recomputes all its graph\n",
    "            # dependencies.\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x68f-hxRGm3H"
   },
   "source": [
    "Let's now switch to stochastic gradient descent training instead, which is much faster.\n",
    "\n",
    "The graph will be similar, except that instead of holding all the training data into a constant node, we create a `Placeholder` node which will be fed actual data at every call of `session.run()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "qhPMzWYRGrzM"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XmVZESmtG4JH"
   },
   "source": [
    "Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 6
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 66292,
     "status": "ok",
     "timestamp": 1449848003013,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "FoF91pknG_YW",
    "outputId": "d255c80e-954d-4183-ca1c-c7333ce91d0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 15.318331\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 13.2%\n",
      "Test accuracy: 13.3%\n",
      "Minibatch loss at step 500: 2.429765\n",
      "Minibatch accuracy: 68.0%\n",
      "Validation accuracy: 75.9%\n",
      "Test accuracy: 83.2%\n",
      "Minibatch loss at step 1000: 1.534487\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 77.1%\n",
      "Test accuracy: 84.5%\n",
      "Minibatch loss at step 1500: 1.385989\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 77.5%\n",
      "Test accuracy: 85.2%\n",
      "Minibatch loss at step 2000: 1.260992\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 77.4%\n",
      "Test accuracy: 85.1%\n",
      "Minibatch loss at step 2500: 1.298825\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 79.3%\n",
      "Test accuracy: 86.0%\n",
      "Minibatch loss at step 3000: 0.639152\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 79.1%\n",
      "Test accuracy: 86.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {\n",
    "            tf_train_dataset : batch_data, \n",
    "            tf_train_labels : batch_labels\n",
    "        }\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7omWxtvLLxik"
   },
   "source": [
    "---\n",
    "Problem\n",
    "-------\n",
    "\n",
    "Turn the logistic regression example with SGD into a 1-hidden layer neural network with rectified linear units [nn.relu()](https://www.tensorflow.org/versions/r0.7/api_docs/python/nn.html#relu) and 1024 hidden nodes. This model should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 784)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def initialize_parameters():\n",
    "    parameters = {}\n",
    "    layers_dims = [ image_size**2, 1024, num_labels ]\n",
    "    \n",
    "    for l in range(1, len(layers_dims)):\n",
    "        parameters['W' + str(l)] = tf.Variable(tf.truncated_normal([layers_dims[l-1], layers_dims[l]]))\n",
    "        parameters['b' + str(l)] = tf.Variable(tf.zeros([layers_dims[l]]))\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    A = X\n",
    "    L = len(parameters) // 2 + 1 # number of layers in the neural network\n",
    "    \n",
    "    # linear[1] -> relu -> liner[2] -> ... -> relu -> linear[L-1]\n",
    "    for l in range(1, L):\n",
    "        W = parameters['W' + str(l)]\n",
    "        b = parameters['b' + str(l)]\n",
    "        Z = tf.add(tf.matmul(A, W), b)\n",
    "        if (l < L - 1):\n",
    "            A = tf.nn.relu(Z)\n",
    "               \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "learning_rate= 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size**2))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset) \n",
    "    \n",
    "    parameters = initialize_parameters()\n",
    "    \n",
    "    logits = forward_propagation(tf_train_dataset, parameters)\n",
    "        \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "      \n",
    "    train_prediction = tf.nn.softmax(forward_propagation(tf_train_dataset, parameters))\n",
    "    valid_prediction = tf.nn.softmax(forward_propagation(tf_valid_dataset, parameters))\n",
    "    test_prediction = tf.nn.softmax(forward_propagation(tf_test_dataset, parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 377.165894\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 25.9%\n",
      "Test accuracy: 27.7%\n",
      "Minibatch loss at step 500: 26.744843\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 78.5%\n",
      "Test accuracy: 86.0%\n",
      "Minibatch loss at step 1000: 13.839665\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 81.5%\n",
      "Test accuracy: 88.2%\n",
      "Minibatch loss at step 1500: 7.114422\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.8%\n",
      "Test accuracy: 88.4%\n",
      "Minibatch loss at step 2000: 8.568427\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 81.6%\n",
      "Test accuracy: 88.2%\n",
      "Minibatch loss at step 2500: 9.817942\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 81.3%\n",
      "Test accuracy: 88.1%\n",
      "Minibatch loss at step 3000: 2.148283\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 82.0%\n",
      "Test accuracy: 88.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "costs = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {\n",
    "            tf_train_dataset : batch_data, \n",
    "            tf_train_labels : batch_labels\n",
    "        }\n",
    "        _, step_cost, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, step_cost))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "        if (step % 5 == 0):\n",
    "            costs.append(step_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xfe0a470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(np.squeeze(costs))\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per tens)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "2_fullyconnected.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
